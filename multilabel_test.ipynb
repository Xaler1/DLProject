{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-30T20:37:05.343050800Z",
     "start_time": "2023-11-30T20:37:00.042464500Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gc\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data_root = os.path.join(\"./\", \"data/\")\n",
    "\n",
    "x_train = np.load(os.path.join(data_root, \"X_train.npy\"))\n",
    "y_train_raw = pd.read_csv(os.path.join(data_root, \"y_train.csv\"), header=None)\n",
    "\n",
    "# convert strings to corresponding arrays\n",
    "y_train_raw[0] = y_train_raw[0].apply(lambda x: ast.literal_eval(x))\n",
    "y_train_raw = y_train_raw[0].values\n",
    "\n",
    "x_test = np.load(os.path.join(data_root, \"X_test.npy\"))\n",
    "y_test_raw = pd.read_csv(os.path.join(data_root, \"y_test.csv\"), header=None)\n",
    "y_test_raw[0] = y_test_raw[0].apply(lambda x: ast.literal_eval(x))\n",
    "y_test_raw = y_test_raw[0].values\n",
    "\n",
    "class_to_index = {\n",
    "    \"NORM\": 0,\n",
    "    \"MI\": 1,\n",
    "    \"HYP\": 2,\n",
    "    \"STTC\": 3,\n",
    "    \"CD\": 4\n",
    "}\n",
    "\n",
    "# Encoding the labels for multi-label classification\n",
    "y_test = torch.zeros((len(y_test_raw), len(class_to_index)), dtype=torch.float32)\n",
    "for i, classification in enumerate(y_test_raw):\n",
    "    for class_name in classification:\n",
    "        y_test[i, class_to_index[class_name]] = 1\n",
    "\n",
    "y_train = torch.zeros((len(y_train_raw), len(class_to_index)), dtype=torch.float32)\n",
    "for i, classification in enumerate(y_train_raw):\n",
    "    for class_name in classification:\n",
    "        y_train[i, class_to_index[class_name]] = 1\n",
    "\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
    "\n",
    "# Free up some memory\n",
    "del y_train_raw\n",
    "del y_test_raw"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T20:37:16.669195700Z",
     "start_time": "2023-11-30T20:37:14.174024600Z"
    }
   },
   "id": "e080b8b33a3a8b96"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_set = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_set = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T20:37:18.399674Z",
     "start_time": "2023-11-30T20:37:18.383162900Z"
    }
   },
   "id": "d29e0e1da2fd7512"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_len=1000, emb_size=12):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, emb_size)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, emb_size, 2).float() * (-np.log(10000.0) / emb_size))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class Transformer(nn.Transformer):\n",
    "    def __init__(self, emb_size=12, nhead=6, depth=6, hidden_size=128, seq_length=1000, num_classes=5):\n",
    "        super(Transformer, self).__init__(d_model=emb_size, nhead=nhead, num_encoder_layers=depth, num_decoder_layers=depth, dim_feedforward=hidden_size)\n",
    "    \n",
    "        self.pos_encoder = PositionalEncoding(seq_length, emb_size)\n",
    "        self.decoder = nn.Linear(emb_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = self.pos_encoder(x)\n",
    "        x = self.encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.decoder(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T20:49:06.138629200Z",
     "start_time": "2023-11-30T20:49:06.130628500Z"
    }
   },
   "id": "5f6ca9bf93a33bf1"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def train(net, optimizer, criterion, train_loader, epochs=10, scheduler=None):\n",
    "    net = net.to(device)\n",
    "\n",
    "    train_losses = []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        pbar = tqdm(train_loader, total=len(train_loader))\n",
    "        last_i = 0\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        for i, (x, y) in enumerate(pbar):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = net(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), 5)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            # exact match ratio\n",
    "            acc = accuracy_score(y.cpu().detach().numpy(), y_pred.cpu().detach().numpy().round())\n",
    "            running_loss += loss.item()\n",
    "            running_acc += acc\n",
    "            \n",
    "            if i % 20 == 1:\n",
    "                running_loss /= (i - last_i)\n",
    "                running_acc /= (i - last_i)\n",
    "                pbar.set_description(f\"loss: {running_loss:.4f}, acc: {running_acc:.4f}\")\n",
    "                running_acc = 0.0\n",
    "                running_loss = 0.0\n",
    "                last_i = i\n",
    "                \n",
    "            \n",
    "            if scheduler is not None:\n",
    "                scheduler.step(loss.item())\n",
    "\n",
    "    return train_losses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T20:52:11.103827200Z",
     "start_time": "2023-11-30T20:52:11.081824800Z"
    }
   },
   "id": "c6fc611fbbc3aa9"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.4975, acc: 0.2547: 100%|██████████| 154/154 [00:17<00:00,  8.85it/s]\n",
      "loss: 0.4475, acc: 0.3453:  69%|██████▉   | 107/154 [00:11<00:05,  8.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00260: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.4535, acc: 0.3328: 100%|██████████| 154/154 [00:17<00:00,  8.97it/s]\n",
      "loss: 0.4460, acc: 0.3559:  36%|███▌      | 55/154 [00:06<00:11,  8.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00362: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.4409, acc: 0.3391: 100%|██████████| 154/154 [00:17<00:00,  8.95it/s]\n",
      "loss: 0.4414, acc: 0.3312:  19%|█▉        | 30/154 [00:03<00:13,  8.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00491: reducing learning rate of group 0 to 1.2500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.4353, acc: 0.3652:  74%|███████▍  | 114/154 [00:12<00:04,  8.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00575: reducing learning rate of group 0 to 6.2500e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.4421, acc: 0.3422: 100%|██████████| 154/154 [00:17<00:00,  8.95it/s]\n",
      "loss: 0.4347, acc: 0.3652:  20%|██        | 31/154 [00:03<00:13,  8.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00646: reducing learning rate of group 0 to 3.1250e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.4312, acc: 0.3582:  66%|██████▌   | 102/154 [00:11<00:05,  8.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00717: reducing learning rate of group 0 to 1.5625e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.4330, acc: 0.3668: 100%|██████████| 154/154 [00:17<00:00,  8.92it/s]\n",
      "loss: 0.8481, acc: 0.7656:  12%|█▏        | 19/154 [00:02<00:15,  8.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00788: reducing learning rate of group 0 to 7.8125e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.4341, acc: 0.3617:  58%|█████▊    | 90/154 [00:10<00:07,  8.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00859: reducing learning rate of group 0 to 3.9063e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.4330, acc: 0.3582: 100%|██████████| 154/154 [00:17<00:00,  8.90it/s]\n",
      "loss: 0.8897, acc: 0.7188:   5%|▍         | 7/154 [00:00<00:16,  8.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00930: reducing learning rate of group 0 to 1.9531e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.4390, acc: 0.3461:  51%|█████     | 78/154 [00:08<00:08,  8.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01001: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.4394, acc: 0.3426: 100%|██████████| 154/154 [00:17<00:00,  8.88it/s]\n",
      "loss: 0.4361, acc: 0.3527: 100%|██████████| 154/154 [00:17<00:00,  8.80it/s]\n",
      "loss: 0.4314, acc: 0.3621: 100%|██████████| 154/154 [00:17<00:00,  8.87it/s]\n",
      "loss: 0.4288, acc: 0.3566: 100%|██████████| 154/154 [00:17<00:00,  8.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.7164759039878845,\n 0.6298056840896606,\n 0.6172776818275452,\n 0.6190471649169922,\n 0.6184773445129395,\n 0.5989018678665161,\n 0.5665106177330017,\n 0.5999088287353516,\n 0.573437511920929,\n 0.5800739526748657,\n 0.5679255127906799,\n 0.5697973966598511,\n 0.5729047656059265,\n 0.5494944453239441,\n 0.5626229047775269,\n 0.5715336799621582,\n 0.5478313565254211,\n 0.5643463730812073,\n 0.5564497113227844,\n 0.5626509785652161,\n 0.5633071064949036,\n 0.5651668906211853,\n 0.5788410902023315,\n 0.5425630807876587,\n 0.5551902055740356,\n 0.5536721348762512,\n 0.5533437132835388,\n 0.5542740821838379,\n 0.5525462031364441,\n 0.5881929397583008,\n 0.5485308766365051,\n 0.5342990159988403,\n 0.5599250197410583,\n 0.5330125093460083,\n 0.5387675166130066,\n 0.5498184561729431,\n 0.5623831152915955,\n 0.5277512669563293,\n 0.5434938669204712,\n 0.5493959784507751,\n 0.538787841796875,\n 0.5687098503112793,\n 0.547488808631897,\n 0.5507074594497681,\n 0.5396263003349304,\n 0.5323004126548767,\n 0.5470455288887024,\n 0.5552889704704285,\n 0.5359945297241211,\n 0.534402072429657,\n 0.5388623476028442,\n 0.5280506610870361,\n 0.5662381052970886,\n 0.5293580889701843,\n 0.5456905364990234,\n 0.5354124307632446,\n 0.5444900393486023,\n 0.5692631006240845,\n 0.5322651267051697,\n 0.5353089570999146,\n 0.5291347503662109,\n 0.5528274774551392,\n 0.5213813185691833,\n 0.5038041472434998,\n 0.519873321056366,\n 0.5559792518615723,\n 0.5259541869163513,\n 0.5399457812309265,\n 0.5426191687583923,\n 0.5401437878608704,\n 0.5221357345581055,\n 0.5314933657646179,\n 0.5527734756469727,\n 0.5313264727592468,\n 0.5105189681053162,\n 0.5361844897270203,\n 0.5413588881492615,\n 0.5389695167541504,\n 0.542952835559845,\n 0.5045046806335449,\n 0.5191048979759216,\n 0.5318796634674072,\n 0.5253849625587463,\n 0.5286831855773926,\n 0.5142664313316345,\n 0.5178154110908508,\n 0.5204032063484192,\n 0.5240580439567566,\n 0.5269116163253784,\n 0.5116193890571594,\n 0.49151450395584106,\n 0.5423266291618347,\n 0.497732937335968,\n 0.5031737685203552,\n 0.5269829034805298,\n 0.5179449915885925,\n 0.5127288699150085,\n 0.49374887347221375,\n 0.5281970500946045,\n 0.4992268979549408,\n 0.5057159662246704,\n 0.49612686038017273,\n 0.520594596862793,\n 0.5083970427513123,\n 0.505824863910675,\n 0.49865642189979553,\n 0.5077246427536011,\n 0.4996797740459442,\n 0.4756554663181305,\n 0.5066555738449097,\n 0.4711306691169739,\n 0.5157060027122498,\n 0.5094980597496033,\n 0.5025045871734619,\n 0.5304054617881775,\n 0.4930499494075775,\n 0.509985625743866,\n 0.47427746653556824,\n 0.49717751145362854,\n 0.49496254324913025,\n 0.5010592341423035,\n 0.4835844933986664,\n 0.5262519121170044,\n 0.505755603313446,\n 0.4879508912563324,\n 0.4687114357948303,\n 0.5215702056884766,\n 0.5032181143760681,\n 0.4757598042488098,\n 0.49883589148521423,\n 0.5180893540382385,\n 0.49619999527931213,\n 0.4995522201061249,\n 0.4883139729499817,\n 0.48805341124534607,\n 0.48388171195983887,\n 0.4985840916633606,\n 0.49879008531570435,\n 0.5060593485832214,\n 0.4912947714328766,\n 0.4885396957397461,\n 0.504586935043335,\n 0.47905197739601135,\n 0.45597338676452637,\n 0.476279079914093,\n 0.47640538215637207,\n 0.48325642943382263,\n 0.4843561351299286,\n 0.49881085753440857,\n 0.4660436809062958,\n 0.45674458146095276,\n 0.5112738609313965,\n 0.5090339779853821,\n 0.4520837366580963,\n 0.4674026072025299,\n 0.49486127495765686,\n 0.4870153069496155,\n 0.4712978005409241,\n 0.4936988055706024,\n 0.4595033824443817,\n 0.49987053871154785,\n 0.45833688974380493,\n 0.4720974862575531,\n 0.4899774193763733,\n 0.46246740221977234,\n 0.4849303364753723,\n 0.47649356722831726,\n 0.46944475173950195,\n 0.49303722381591797,\n 0.4804893434047699,\n 0.45015621185302734,\n 0.4826807975769043,\n 0.47633934020996094,\n 0.48639851808547974,\n 0.4658678472042084,\n 0.45502766966819763,\n 0.4615623652935028,\n 0.4740849435329437,\n 0.48403969407081604,\n 0.4609602391719818,\n 0.489035427570343,\n 0.462709903717041,\n 0.4690015912055969,\n 0.47118568420410156,\n 0.4642413556575775,\n 0.4487302899360657,\n 0.49024295806884766,\n 0.49320417642593384,\n 0.4106867015361786,\n 0.4398391842842102,\n 0.44853806495666504,\n 0.47929954528808594,\n 0.4590480923652649,\n 0.41586071252822876,\n 0.4673447608947754,\n 0.46598950028419495,\n 0.4469485878944397,\n 0.4856348931789398,\n 0.434873104095459,\n 0.4329984188079834,\n 0.5083181262016296,\n 0.4644874632358551,\n 0.4805453419685364,\n 0.4853864312171936,\n 0.449910968542099,\n 0.4426354467868805,\n 0.48364779353141785,\n 0.43451032042503357,\n 0.40976133942604065,\n 0.49581989645957947,\n 0.43665289878845215,\n 0.4755551517009735,\n 0.4778769910335541,\n 0.4637368321418762,\n 0.4738104045391083,\n 0.4831104874610901,\n 0.4692014753818512,\n 0.4157371520996094,\n 0.44672489166259766,\n 0.49569231271743774,\n 0.44766148924827576,\n 0.491487592458725,\n 0.46282634139060974,\n 0.43954306840896606,\n 0.45676133036613464,\n 0.43486499786376953,\n 0.4693625867366791,\n 0.48162370920181274,\n 0.44010135531425476,\n 0.4322687089443207,\n 0.4485692083835602,\n 0.46111613512039185,\n 0.4696694016456604,\n 0.43907538056373596,\n 0.454073429107666,\n 0.44881802797317505,\n 0.46155738830566406,\n 0.4182479977607727,\n 0.4332672655582428,\n 0.4398646354675293,\n 0.43342313170433044,\n 0.413120836019516,\n 0.4983682632446289,\n 0.4807860553264618,\n 0.46534833312034607,\n 0.43578097224235535,\n 0.46523937582969666,\n 0.4424026608467102,\n 0.470194548368454,\n 0.42443713545799255,\n 0.4527909457683563,\n 0.45566245913505554,\n 0.45000505447387695,\n 0.43938422203063965,\n 0.4347824156284332,\n 0.43615293502807617,\n 0.41976186633110046,\n 0.43105050921440125,\n 0.4339756965637207,\n 0.428617924451828,\n 0.451045423746109,\n 0.4425596296787262,\n 0.49337640404701233,\n 0.4391006529331207,\n 0.41675764322280884,\n 0.4515174925327301,\n 0.484283447265625,\n 0.4927116930484772,\n 0.4686341881752014,\n 0.44684362411499023,\n 0.46857768297195435,\n 0.4278066158294678,\n 0.4807174801826477,\n 0.4751379191875458,\n 0.4618590772151947,\n 0.44913169741630554,\n 0.47883009910583496,\n 0.43273457884788513,\n 0.466463178396225,\n 0.4382384419441223,\n 0.455310195684433,\n 0.4464583396911621,\n 0.4459439218044281,\n 0.45494863390922546,\n 0.4536014497280121,\n 0.45795097947120667,\n 0.4066338539123535,\n 0.45538949966430664,\n 0.4857816696166992,\n 0.46348562836647034,\n 0.47716543078422546,\n 0.44507789611816406,\n 0.4828626811504364,\n 0.4582829177379608,\n 0.44947654008865356,\n 0.4155389964580536,\n 0.47040826082229614,\n 0.4482673704624176,\n 0.45741215348243713,\n 0.4691416919231415,\n 0.4601520597934723,\n 0.45965471863746643,\n 0.40369948744773865,\n 0.4285409152507782,\n 0.48236772418022156,\n 0.4269884526729584,\n 0.43930235505104065,\n 0.4212765097618103,\n 0.4058980643749237,\n 0.42222434282302856,\n 0.4003125727176666,\n 0.46047648787498474,\n 0.44595345854759216,\n 0.4560171663761139,\n 0.44704172015190125,\n 0.45134496688842773,\n 0.47588101029396057,\n 0.4642191529273987,\n 0.4511548578739166,\n 0.45038866996765137,\n 0.44061079621315,\n 0.45317989587783813,\n 0.44197702407836914,\n 0.45612964034080505,\n 0.46591687202453613,\n 0.4403671324253082,\n 0.43593311309814453,\n 0.43064603209495544,\n 0.4754696786403656,\n 0.4412444233894348,\n 0.4541608989238739,\n 0.45097842812538147,\n 0.4173368513584137,\n 0.41942882537841797,\n 0.45370393991470337,\n 0.4445830285549164,\n 0.4644829332828522,\n 0.44823575019836426,\n 0.48549994826316833,\n 0.44385242462158203,\n 0.46782350540161133,\n 0.43972840905189514,\n 0.43320465087890625,\n 0.41615158319473267,\n 0.4532725512981415,\n 0.45065927505493164,\n 0.4477247893810272,\n 0.4752208888530731,\n 0.41634806990623474,\n 0.43690767884254456,\n 0.4787778854370117,\n 0.41604098677635193,\n 0.4306796193122864,\n 0.48497095704078674,\n 0.4651031494140625,\n 0.4397338032722473,\n 0.43401607871055603,\n 0.44734010100364685,\n 0.42562124133110046,\n 0.45406776666641235,\n 0.40890318155288696,\n 0.4325098991394043,\n 0.4403371810913086,\n 0.4207886755466461,\n 0.42883172631263733,\n 0.4294903874397278,\n 0.4625821113586426,\n 0.44890061020851135,\n 0.4097914397716522,\n 0.4416024386882782,\n 0.45477914810180664,\n 0.40609073638916016,\n 0.4377903938293457,\n 0.43190789222717285,\n 0.4561585485935211,\n 0.41259804368019104,\n 0.447386234998703,\n 0.4517369866371155,\n 0.4822057783603668,\n 0.41205206513404846,\n 0.4271600842475891,\n 0.4365202486515045,\n 0.44735392928123474,\n 0.4620419442653656,\n 0.42606136202812195,\n 0.46903353929519653,\n 0.43790513277053833,\n 0.4222433269023895,\n 0.46096062660217285,\n 0.43883243203163147,\n 0.40447503328323364,\n 0.43686777353286743,\n 0.46694323420524597,\n 0.4748530387878418,\n 0.47003647685050964,\n 0.4622248709201813,\n 0.4760485589504242,\n 0.439179390668869,\n 0.4503324627876282,\n 0.4503210186958313,\n 0.44206705689430237,\n 0.4412607252597809,\n 0.47737836837768555,\n 0.4235057830810547,\n 0.4337332844734192,\n 0.4406079947948456,\n 0.4454549252986908,\n 0.43383827805519104,\n 0.46499910950660706,\n 0.4179837703704834,\n 0.3933107554912567,\n 0.472611665725708,\n 0.43156203627586365,\n 0.4226153492927551,\n 0.4549635052680969,\n 0.4517819881439209,\n 0.4377416670322418,\n 0.4391389489173889,\n 0.4375283718109131,\n 0.4878961741924286,\n 0.4314676821231842,\n 0.451884001493454,\n 0.4544282555580139,\n 0.44677719473838806,\n 0.43223634362220764,\n 0.4426698684692383,\n 0.413495272397995,\n 0.42401954531669617,\n 0.43981677293777466,\n 0.4244318902492523,\n 0.4563736915588379,\n 0.4678761959075928,\n 0.4109009802341461,\n 0.48606738448143005,\n 0.4643217623233795,\n 0.43941640853881836,\n 0.47743502259254456,\n 0.4054320454597473,\n 0.4397263526916504,\n 0.3903687298297882,\n 0.4655434787273407,\n 0.45020434260368347,\n 0.4331648051738739,\n 0.44517752528190613,\n 0.4155143201351166,\n 0.44117632508277893,\n 0.424864262342453,\n 0.4522233009338379,\n 0.4196583926677704,\n 0.4327397346496582,\n 0.40703049302101135,\n 0.4493788778781891,\n 0.4288308322429657,\n 0.45533210039138794,\n 0.45715102553367615,\n 0.39781150221824646,\n 0.4538908004760742,\n 0.4460037350654602,\n 0.4429030418395996,\n 0.4361763894557953,\n 0.3923308551311493,\n 0.39109310507774353,\n 0.45267006754875183,\n 0.40817466378211975,\n 0.43563365936279297,\n 0.4784819781780243,\n 0.4300578534603119,\n 0.4305606782436371,\n 0.4670257568359375,\n 0.4470643103122711,\n 0.4377956986427307,\n 0.4922513961791992,\n 0.44927874207496643,\n 0.4159514009952545,\n 0.445372074842453,\n 0.4461313784122467,\n 0.42378804087638855,\n 0.4371863305568695,\n 0.4270510673522949,\n 0.426741361618042,\n 0.407457560300827,\n 0.4125290513038635,\n 0.44203710556030273,\n 0.4752658009529114,\n 0.4114570617675781,\n 0.43359920382499695,\n 0.4218720495700836,\n 0.46934422850608826,\n 0.4227076470851898,\n 0.4191009998321533,\n 0.4628243148326874,\n 0.4414171278476715,\n 0.4412499964237213,\n 0.4307170510292053,\n 0.44170957803726196,\n 0.4370778203010559,\n 0.4161128103733063,\n 0.3963603079319,\n 0.43943101167678833,\n 0.4424695670604706,\n 0.40634885430336,\n 0.4542694091796875,\n 0.4411807060241699,\n 0.4535495936870575,\n 0.4328768849372864,\n 0.42008882761001587,\n 0.40763911604881287,\n 0.42067986726760864,\n 0.45167094469070435,\n 0.4313949644565582,\n 0.4173378646373749,\n 0.4155428111553192,\n 0.44788381457328796,\n 0.4068317115306854,\n 0.39491006731987,\n 0.39373454451560974,\n 0.4458179473876953,\n 0.45289379358291626,\n 0.43719562888145447,\n 0.4434987008571625,\n 0.4037923812866211,\n 0.431882381439209,\n 0.481204092502594,\n 0.3865085244178772,\n 0.4307670295238495,\n 0.43749675154685974,\n 0.4661998748779297,\n 0.47941747307777405,\n 0.4136483371257782,\n 0.4668141007423401,\n 0.48040637373924255,\n 0.43067407608032227,\n 0.4326968789100647,\n 0.42865076661109924,\n 0.44372984766960144,\n 0.47051358222961426,\n 0.449005126953125,\n 0.4416359066963196,\n 0.4612088203430176,\n 0.3924756646156311,\n 0.46041589975357056,\n 0.41405317187309265,\n 0.40671977400779724,\n 0.43801209330558777,\n 0.4551020562648773,\n 0.3924085795879364,\n 0.45944079756736755,\n 0.4305935800075531,\n 0.4457230567932129,\n 0.4377572238445282,\n 0.42136150598526,\n 0.4521801471710205,\n 0.4200446307659149,\n 0.47304391860961914,\n 0.44386911392211914,\n 0.42144712805747986,\n 0.44058623909950256,\n 0.39287981390953064,\n 0.4342409074306488,\n 0.4238239824771881,\n 0.41500696539878845,\n 0.451753705739975,\n 0.4429461658000946,\n 0.4514492452144623,\n 0.45636796951293945,\n 0.426882266998291,\n 0.44845348596572876,\n 0.41417479515075684,\n 0.4134068191051483,\n 0.43266478180885315,\n 0.46086636185646057,\n 0.4483344554901123,\n 0.43449392914772034,\n 0.4262003004550934,\n 0.43985292315483093,\n 0.4440343976020813,\n 0.46903857588768005,\n 0.4225897789001465,\n 0.466022789478302,\n 0.43963494896888733,\n 0.47696706652641296,\n 0.41555434465408325,\n 0.4331657886505127,\n 0.4596732258796692,\n 0.4552084505558014,\n 0.4482041001319885,\n 0.4383593499660492,\n 0.44425860047340393,\n 0.4207979142665863,\n 0.4518956243991852,\n 0.49153923988342285,\n 0.4452846646308899,\n 0.4208512306213379,\n 0.4395289123058319,\n 0.4562826156616211,\n 0.46500664949417114,\n 0.4461895525455475,\n 0.41797977685928345,\n 0.4319467544555664,\n 0.41719600558280945,\n 0.44196388125419617,\n 0.4303664267063141,\n 0.42763328552246094,\n 0.4518185257911682,\n 0.421305388212204,\n 0.39931192994117737,\n 0.46211671829223633,\n 0.47736769914627075,\n 0.42330923676490784,\n 0.41377171874046326,\n 0.44230398535728455,\n 0.39222511649131775,\n 0.44302454590797424,\n 0.4070969223976135,\n 0.4718690812587738,\n 0.43378910422325134,\n 0.41272279620170593,\n 0.4141372740268707,\n 0.40582847595214844,\n 0.4152807295322418,\n 0.4552628993988037,\n 0.42016226053237915,\n 0.4391968250274658,\n 0.43859702348709106,\n 0.4315033555030823,\n 0.45589980483055115,\n 0.4573056399822235,\n 0.43879634141921997,\n 0.40537285804748535,\n 0.4485030174255371,\n 0.4564708173274994,\n 0.4743098318576813,\n 0.4366445541381836,\n 0.4185499846935272,\n 0.40680739283561707,\n 0.425829142332077,\n 0.42019549012184143,\n 0.4432920515537262,\n 0.4551234245300293,\n 0.43137726187705994,\n 0.4313942492008209,\n 0.3983686566352844,\n 0.46658119559288025,\n 0.43923941254615784,\n 0.45297566056251526,\n 0.41070204973220825,\n 0.420208215713501,\n 0.46882468461990356,\n 0.44862768054008484,\n 0.4696815609931946,\n 0.4392103850841522,\n 0.4397178590297699,\n 0.41992005705833435,\n 0.4494756758213043,\n 0.42834949493408203,\n 0.4207738935947418,\n 0.44570961594581604,\n 0.43609094619750977,\n 0.46402707695961,\n 0.4474106729030609,\n 0.4358367919921875,\n 0.4233565032482147,\n 0.4280293583869934,\n 0.43216753005981445,\n 0.4298498332500458,\n 0.45084142684936523,\n 0.4611378312110901,\n 0.43865880370140076,\n 0.4283019006252289,\n 0.44110313057899475,\n 0.4655682146549225,\n 0.4581579267978668,\n 0.394841730594635,\n 0.4703558385372162,\n 0.42610421776771545,\n 0.43571099638938904,\n 0.42783141136169434,\n 0.41555672883987427,\n 0.42498454451560974,\n 0.43204569816589355,\n 0.43877092003822327,\n 0.43170782923698425,\n 0.4418564736843109,\n 0.44528913497924805,\n 0.46723443269729614,\n 0.417248010635376,\n 0.4424547255039215,\n 0.46126842498779297,\n 0.43674716353416443,\n 0.444917768239975,\n 0.4442899227142334,\n 0.4133985638618469,\n 0.47419843077659607,\n 0.46412086486816406,\n 0.44259461760520935,\n 0.4365273416042328,\n 0.4719887673854828,\n 0.440723717212677,\n 0.42752328515052795,\n 0.42380523681640625,\n 0.43007776141166687,\n 0.41932231187820435,\n 0.4495278000831604,\n 0.4493423402309418,\n 0.4406304359436035,\n 0.41896986961364746,\n 0.4106176793575287,\n 0.4177706837654114,\n 0.44703084230422974,\n 0.41894012689590454,\n 0.4530709683895111,\n 0.4361099302768707,\n 0.42948323488235474,\n 0.42068204283714294,\n 0.3945377767086029,\n 0.43243178725242615,\n 0.41798457503318787,\n 0.4858575761318207,\n 0.4205358922481537,\n 0.393807053565979,\n 0.4117654860019684,\n 0.43684902787208557,\n 0.4855356216430664,\n 0.44831475615501404,\n 0.4411432445049286,\n 0.4108901023864746,\n 0.41295501589775085,\n 0.3998919129371643,\n 0.4007073938846588,\n 0.45334455370903015,\n 0.41409870982170105,\n 0.43452712893486023,\n 0.4255598485469818,\n 0.40435799956321716,\n 0.40872010588645935,\n 0.41412124037742615,\n 0.43852758407592773,\n 0.48166218400001526,\n 0.45776844024658203,\n 0.42348533868789673,\n 0.4423002302646637,\n 0.4000721871852875,\n 0.4571240544319153,\n 0.4419487416744232,\n 0.4125283360481262,\n 0.3936786651611328,\n 0.4412136971950531,\n 0.430195152759552,\n 0.45099711418151855,\n 0.4428198039531708,\n 0.442028284072876,\n 0.4203552305698395,\n 0.43204933404922485,\n 0.43721523880958557,\n 0.46080732345581055,\n 0.4155740737915039,\n 0.41767263412475586,\n 0.4398033618927002,\n 0.43487825989723206,\n 0.48565560579299927,\n 0.4199867844581604,\n 0.43754157423973083,\n 0.4002145230770111,\n 0.4156031608581543,\n 0.43283459544181824,\n 0.42957115173339844,\n 0.46438199281692505,\n 0.4400182366371155,\n 0.4495634138584137,\n 0.46953916549682617,\n 0.4197043478488922,\n 0.4284364879131317,\n 0.42848512530326843,\n 0.392914354801178,\n 0.4609043300151825,\n 0.4263482093811035,\n 0.4132086932659149,\n 0.4707331657409668,\n 0.46097517013549805,\n 0.4074014723300934,\n 0.4173892140388489,\n 0.4520236551761627,\n 0.4005691707134247,\n 0.4170388877391815,\n 0.45593634247779846,\n 0.4388323426246643,\n 0.47629618644714355,\n 0.3939349055290222,\n 0.41555437445640564,\n 0.41535684466362,\n 0.45177754759788513,\n 0.4432081878185272,\n 0.4401451647281647,\n 0.41963091492652893,\n 0.45289722084999084,\n 0.42329713702201843,\n 0.43926429748535156,\n 0.46492981910705566,\n 0.43960896134376526,\n 0.43533816933631897,\n 0.4617321491241455,\n 0.43087372183799744,\n 0.47322407364845276,\n 0.4388645589351654,\n 0.46333399415016174,\n 0.4254426956176758,\n 0.4442184865474701,\n 0.4207555949687958,\n 0.4359498918056488,\n 0.4146469533443451,\n 0.43771764636039734,\n 0.4316798150539398,\n 0.44828280806541443,\n 0.4470803439617157,\n 0.407908171415329,\n 0.41672107577323914,\n 0.4442236125469208,\n 0.43202853202819824,\n 0.4482034742832184,\n 0.41263270378112793,\n 0.43006211519241333,\n 0.4227313995361328,\n 0.44929176568984985,\n 0.39272236824035645,\n 0.45007917284965515,\n 0.4136272370815277,\n 0.4470894932746887,\n 0.4242405891418457,\n 0.4521598815917969,\n 0.4597361087799072,\n 0.4726112484931946,\n 0.4412747919559479,\n 0.4615168571472168,\n 0.46584731340408325,\n 0.4684167802333832,\n 0.42780962586402893,\n 0.4328162670135498,\n 0.4146021902561188,\n 0.4098876118659973,\n 0.4242863357067108,\n 0.43583598732948303,\n 0.45586252212524414,\n 0.45204901695251465,\n 0.4136066436767578,\n 0.4574849307537079,\n 0.4078054428100586,\n 0.4319719970226288,\n 0.44189754128456116,\n 0.4446234405040741,\n 0.40304967761039734,\n 0.40783047676086426,\n 0.4238083064556122,\n 0.4488646686077118,\n 0.4397413730621338,\n 0.4550026059150696,\n 0.4579050540924072,\n 0.4277116358280182,\n 0.41190728545188904,\n 0.41251054406166077,\n 0.4048195481300354,\n 0.44270116090774536,\n 0.4294058382511139,\n 0.44291672110557556,\n 0.41255807876586914,\n 0.4415837228298187,\n 0.45217496156692505,\n 0.43494507670402527,\n 0.47174063324928284,\n 0.42413169145584106,\n 0.4352266490459442,\n 0.42050838470458984,\n 0.4431777894496918,\n 0.42691096663475037,\n 0.4436415731906891,\n 0.3982529044151306,\n 0.43491607904434204,\n 0.4321436583995819,\n 0.42791396379470825,\n 0.41098910570144653,\n 0.4479873776435852,\n 0.4426397979259491,\n 0.3899077773094177,\n 0.44939231872558594,\n 0.4306642711162567,\n 0.3982616364955902,\n 0.443742036819458,\n 0.43623247742652893,\n 0.42673826217651367,\n 0.4828186631202698,\n 0.43164119124412537,\n 0.42237135767936707,\n 0.42042475938796997,\n 0.4428706169128418,\n 0.4305651783943176,\n 0.458716481924057,\n 0.441715806722641,\n 0.43362799286842346,\n 0.4315083920955658,\n 0.41537413001060486,\n 0.4208029806613922,\n 0.4453197419643402,\n 0.4063391387462616,\n 0.4167264997959137,\n 0.4399428069591522,\n 0.4483015239238739,\n 0.4360693097114563,\n 0.4512999653816223,\n 0.43165189027786255,\n 0.43834978342056274,\n 0.43852677941322327,\n 0.41635504364967346,\n 0.4161298871040344,\n 0.41645365953445435,\n 0.43923497200012207,\n 0.49508142471313477,\n 0.4479810297489166,\n 0.44608813524246216,\n 0.4131646156311035,\n 0.443746954202652,\n 0.4536004960536957,\n 0.4639599323272705,\n 0.4134359359741211,\n 0.42944571375846863,\n 0.44623109698295593,\n 0.44133463501930237,\n 0.44835400581359863,\n 0.43230053782463074,\n 0.4319216310977936,\n 0.4310799539089203,\n 0.43072113394737244,\n 0.4034389555454254,\n 0.37632957100868225,\n 0.41681623458862305,\n 0.4182264804840088,\n 0.4577300548553467,\n 0.4707392752170563,\n 0.4216165542602539,\n 0.4537253975868225,\n 0.442208856344223,\n 0.39680230617523193,\n 0.43832966685295105,\n 0.4231499135494232,\n 0.4320872724056244,\n 0.40896740555763245,\n 0.39429473876953125,\n 0.4167884886264801,\n 0.43645867705345154,\n 0.432366281747818,\n 0.4158147871494293,\n 0.41784366965293884,\n 0.4510361850261688,\n 0.43851229548454285,\n 0.4355405271053314,\n 0.44242653250694275,\n 0.41705021262168884,\n 0.4143531918525696,\n 0.43851253390312195,\n 0.43827030062675476,\n 0.43137598037719727,\n 0.42502012848854065,\n 0.4228942394256592,\n 0.43061238527297974,\n 0.41807135939598083,\n 0.41945719718933105,\n 0.4308179020881653,\n 0.4339735209941864,\n 0.4726076126098633,\n 0.4232785403728485,\n 0.4443623721599579,\n 0.46124520897865295,\n 0.42463618516921997,\n 0.3906562924385071,\n 0.4670639932155609,\n 0.42234888672828674,\n 0.469087690114975,\n 0.4373534321784973,\n 0.43336525559425354,\n 0.4303775429725647,\n 0.43234068155288696,\n 0.46469640731811523,\n 0.4121020436286926,\n 0.42538735270500183,\n 0.48319610953330994,\n 0.4281444549560547,\n 0.43262824416160583,\n 0.42574116587638855,\n 0.4389840066432953,\n 0.4475974142551422,\n 0.40107184648513794,\n 0.44675442576408386,\n 0.44726940989494324,\n 0.3990584909915924,\n 0.44823789596557617,\n 0.42538315057754517,\n 0.4357233941555023,\n 0.43017083406448364,\n 0.438829243183136,\n 0.4150003492832184,\n 0.4445801377296448,\n 0.46724435687065125,\n ...]"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "net = Transformer(nhead=6, hidden_size=512, depth=4)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=50, verbose=True, cooldown=20, factor=0.5, min_lr=1e-6)\n",
    "train(net, optimizer, criterion, train_loader, epochs=10, scheduler=scheduler)\n",
    "print(\"done\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T20:55:31.343583600Z",
     "start_time": "2023-11-30T20:52:38.182478500Z"
    }
   },
   "id": "45a319ed4f13ef0e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T20:48:11.961454200Z"
    }
   },
   "id": "391e1c2967f9a5ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5be64e10f86524a3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
